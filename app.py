import logging
import os
import re

import jieba
import jieba.posseg as pseg
import torch
from flask import Flask, request, jsonify
from gensim.models import KeyedVectors
from transformers import BertTokenizer, BertModel

app = Flask(__name__)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
model_w2v = None
tokenizer = None
bert_model = None
BRANDS = set()
SERVICE_TYPES = set()
ATTRIBUTES = set()
AREAS = set()


# ğŸ“š è‡ªå®šä¹‰è¯å…¸åŠ è½½
def load_dict(file_path):
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                return set(word.strip() for word in f if word.strip())
        else:
            logger.warning(f"è¯å…¸æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è¯å…¸")
            return set()
    except Exception as e:
        logger.error(f"åŠ è½½è¯å…¸æ–‡ä»¶å¤±è´¥: {e}")
        return set()


# ğŸ§¹ é¢„å¤„ç†æœç´¢è¯­å¥
def preprocess_query(query):
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬ç¬¦å·
    query = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\-~è‡³åˆ°å…ƒä¸‡]', '', query)
    return query.strip().lower()


# ğŸ§  BERT å¯¹æ•´å¥ç”Ÿæˆå¥å‘é‡
def get_bert_sentence_embedding(query):
    try:
        if tokenizer and bert_model:
            inputs = tokenizer(query, return_tensors="pt", padding=True, truncation=True, max_length=128)
            with torch.no_grad():
                outputs = bert_model(**inputs)
            sentence_vector = torch.mean(outputs.last_hidden_state, dim=1)
            return sentence_vector.numpy().flatten().tolist()[:10]
        else:
            return [len(query), query.count('çš„'), query.count('åŒº'), len(jieba.lcut(query))]
    except Exception as e:
        logger.error(f"BERTå¥å‘é‡æå–å¤±è´¥: {e}")
        return [len(query), query.count('çš„'), query.count('åŒº'), len(jieba.lcut(query))]


# ğŸª™ åŒä¹‰è¯æ‰©å±•ï¼ˆWord2Vecï¼‰
def get_similar_words(word, model, topn=3):
    try:
        if model and word in model:
            return [w for w, _ in model.most_similar(word, topn=topn)]
        return []
    except Exception as e:
        logger.error(f"è·å–åŒä¹‰è¯å¤±è´¥: {e}")
        return []


# ğŸ“ æå–å¹¶ç»“æ„åŒ–å…³é”®è¯
def extract_keywords(query):
    query = preprocess_query(query)
    words = pseg.lcut(query)

    keywords = {
        "èŒƒå›´": [],
        "å“ç‰Œ": [],
        "æ¡£æ¬¡": [],
        "åˆ†åº—": [],
        "ç±»å‹": [],
        "åœ°å€": [],
        "æ¨¡å¼": [],
        "ä»·æ ¼": None,
        "ä»·æ ¼æ¡ä»¶": None,
        "è¯­ä¹‰æ‰©å±•": {
            "å“ç‰Œæ‰©å±•": [],
            "æ¡£æ¬¡æ‰©å±•": [],
            "æ¨¡å¼æ‰©å±•": []
        },
        "å¥å‘é‡": []
    }

    # åŸºç¡€å…³é”®è¯æå–
    for word, flag in words:
        if word in AREAS:
            keywords['èŒƒå›´'].append(word)
            keywords['åœ°å€'].append(word)
        elif word in BRANDS:
            keywords['å“ç‰Œ'].append(word)
        elif word in SERVICE_TYPES:
            keywords['ç±»å‹'].append(word)
        elif word in ATTRIBUTES:
            if word in ['é«˜æ¡£', 'è±ªå', 'ç»æµå‹', 'ä¸­æ¡£', 'è¶…é«˜æ¡£']:
                keywords['æ¡£æ¬¡'].append(word)
            elif word in ['ä¸“ä¸š', 'å…¨èŒä¸“å®¶', '24å°æ—¶æŠ¤ç†', 'ä¸€å¯¹ä¸€', 'å°å¼æŠ¤ç†', 'æ¯å©´åŒå®¤']:
                keywords['æ¨¡å¼'].append(word)
        elif flag == "m" and word.isdigit():
            keywords['ä»·æ ¼'] = int(word)

    # ä½¿ç”¨æ­£åˆ™è¾…åŠ©æå–ä»·æ ¼
    if keywords['ä»·æ ¼'] is None:
        price_patterns = [
            (r'(\d+)ä¸‡?[\-~è‡³åˆ°](\d+)ä¸‡?', 'range'),
            (r'(?:æœ€å¤š|ä¸è¶…è¿‡|ä¹‹å†…|ä½äº|ä»¥ä¸‹|æ§åˆ¶åœ¨)(?:.*?)(\d+)ä¸‡?', 'lt'),
            (r'(?:é«˜äº|è¶…è¿‡|ä»¥ä¸Š)(?:.*?)(\d+)ä¸‡?', 'gt'),
            (r'(\d+)(?:ä¸‡|å…ƒ)', 'exact')
        ]

        for pattern, condition in price_patterns:
            match = re.search(pattern, query)
            if match:
                if condition == 'range':
                    keywords['ä»·æ ¼æ¡ä»¶'] = 'range'
                    min_price = int(match.group(1))
                    max_price = int(match.group(2))
                    # å¤„ç†ä¸‡å…ƒå•ä½
                    if 'ä¸‡' in match.group(0):
                        min_price *= 10000
                        max_price *= 10000
                    keywords['ä»·æ ¼'] = {"min": min_price, "max": max_price}
                elif condition in ['lt', 'gt', 'exact']:
                    keywords['ä»·æ ¼æ¡ä»¶'] = condition
                    price = int(match.group(1))
                    if 'ä¸‡' in match.group(0):
                        price *= 10000
                    if condition == 'lt':
                        keywords['ä»·æ ¼'] = {"min": 0, "max": price}
                        keywords['ä»·æ ¼æ¡ä»¶'] = 'range'
                    else:
                        keywords['ä»·æ ¼'] = price
                break

    # è¯­ä¹‰æ‰©å±•å…³é”®è¯
    if model_w2v:
        for brand in keywords['å“ç‰Œ']:
            keywords['è¯­ä¹‰æ‰©å±•']['å“ç‰Œæ‰©å±•'].extend(get_similar_words(brand, model_w2v))
        for grade in keywords['æ¡£æ¬¡']:
            keywords['è¯­ä¹‰æ‰©å±•']['æ¡£æ¬¡æ‰©å±•'].extend(get_similar_words(grade, model_w2v))
        for mode in keywords['æ¨¡å¼']:
            keywords['è¯­ä¹‰æ‰©å±•']['æ¨¡å¼æ‰©å±•'].extend(get_similar_words(mode, model_w2v))

    # å»é‡
    keywords['è¯­ä¹‰æ‰©å±•']['å“ç‰Œæ‰©å±•'] = list(set(keywords['è¯­ä¹‰æ‰©å±•']['å“ç‰Œæ‰©å±•']))
    keywords['è¯­ä¹‰æ‰©å±•']['æ¡£æ¬¡æ‰©å±•'] = list(set(keywords['è¯­ä¹‰æ‰©å±•']['æ¡£æ¬¡æ‰©å±•']))
    keywords['è¯­ä¹‰æ‰©å±•']['æ¨¡å¼æ‰©å±•'] = list(set(keywords['è¯­ä¹‰æ‰©å±•']['æ¨¡å¼æ‰©å±•']))

    # æå–å¥å‘é‡
    keywords['å¥å‘é‡'] = get_bert_sentence_embedding(query)

    return keywords


# ğŸ”— API è·¯ç”±
@app.route("/parse", methods=["POST"])
def parse_query():
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "è¯·æ±‚ä½“å¿…é¡»æ˜¯JSONæ ¼å¼"}), 400

        query_text = data.get("query")
        if not query_text:
            return jsonify({"error": "æœªæä¾›æŸ¥è¯¢å†…å®¹ï¼Œè¯·ä¼ å…¥ 'query'"}), 400

        result = extract_keywords(query_text)
        return jsonify(result)
    except Exception as e:
        logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}), 500


@app.route("/health", methods=["GET"])
def health_check():
    return jsonify({"status": "healthy", "message": "æœˆå­ä¸­å¿ƒæŸ¥è¯¢APIæœåŠ¡è¿è¡Œæ­£å¸¸"})


# ğŸš€ åˆå§‹åŒ–å‡½æ•°
def initialize_models():
    global model_w2v, tokenizer, bert_model, BRANDS, SERVICE_TYPES, ATTRIBUTES, AREAS

    logger.info("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹å’Œè¯å…¸...")

    # åŠ è½½è‡ªå®šä¹‰è¯å…¸
    BRANDS = load_dict("./data/brands.txt")
    SERVICE_TYPES = load_dict("./data/service_types.txt")
    ATTRIBUTES = load_dict("./data/attributes.txt")
    AREAS = load_dict("./data/areas.txt")

    # æ·»åŠ é»˜è®¤è¯å…¸ï¼ˆå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼‰
    if not BRANDS:
        BRANDS = {"çˆ±å¸å®«", "é¦¨æœˆæ±‡", "ç¾åå¦‡å„¿", "åœ£è´æ‹‰", "å–œæœˆ", "æœˆå­å°è±¡"}
    if not SERVICE_TYPES:
        SERVICE_TYPES = {"é…’åº—å¼", "å®¶åº­å¼", "åŒ»é™¢å¼", "å…¬å¯“å¼", "åˆ«å¢…å¼"}
    if not ATTRIBUTES:
        ATTRIBUTES = {"é«˜æ¡£", "è±ªå", "ç»æµå‹", "ä¸­æ¡£", "ä¸“ä¸š", "å…¨èŒä¸“å®¶", "24å°æ—¶æŠ¤ç†"}
    if not AREAS:
        AREAS = {"é¼“æ¥¼åŒº", "ç„æ­¦åŒº", "ç§¦æ·®åŒº", "å»ºé‚ºåŒº", "é›¨èŠ±å°åŒº", "æ –éœåŒº"}

    # åŠ è½½jiebaç”¨æˆ·è¯å…¸
    try:
        if os.path.exists("custom_words.txt"):
            jieba.load_userdict("custom_words.txt")
            logger.info("jiebaç”¨æˆ·è¯å…¸åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.warning(f"jiebaç”¨æˆ·è¯å…¸åŠ è½½å¤±è´¥: {e}")

    # å°è¯•åŠ è½½Word2Vecæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    try:
        w2v_model_path = "models/chinese-w2v.kv"
        if os.path.exists(w2v_model_path):
            model_w2v = KeyedVectors.load(w2v_model_path)
            logger.info("Word2Vecæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            logger.info("Word2Vecæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½")
    except Exception as e:
        logger.warning(f"Word2Vecæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    # å°è¯•åŠ è½½BERTæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    try:
        tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
        bert_model = BertModel.from_pretrained("bert-base-chinese")
        logger.info("BERTæ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.warning(f"BERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„ç‰¹å¾æå–")

    logger.info("æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")


# ğŸšª å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    initialize_models()
    app.run(host="0.0.0.0", port=5000, debug=False)
